{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 - Applying ML to Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:03:31\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#change the 'basepath' to the directory of the unzipped movie dataset\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos' : 1, 'neg' : 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file),\n",
    "                     'r', encoding ='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]],\n",
    "                          ignore_index = True)\n",
    "            pbar.update()\n",
    "\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis\n",
    "Sentiment analysis (or opinion mining) is a popular subdiscipline of NLP and is concerned with analyzing the polarity of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv', encoding = 'utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words model\n",
    "\n",
    "A model to represent text as numerical feature vectors.\n",
    "* 1) Create a vocabulary of unique tokens - for example, words = from the entire set of documents\n",
    "* 2) Construct a feature vector from each document that contains the counts of how often each word occurs in the particular document.\n",
    "\n",
    "This will yield a sparse vector. We can do this by using the CountVectorizer() class from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "    'the sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining, the weather is sweet, and one and one is two'\n",
    "])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the above vector represent the raw term frequencies: tf(t,d) - the number of times a term t occurs in a document d.\n",
    "\n",
    "The above created bag-of-words is also called the 1-gram or unigram model (each item represents a single word). The contiguous sequences of items in NLP - words, letters or symbols - are also called n-grams. The coice of the number n in the n-gram model depends on the particular application (i.e. 3/4-grams yield good performances in anti-spam filtering. Can be done via the ngram_range parameter in CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word relevancy via term frequency\n",
    "\n",
    "Some words occur often accross both negative and positive classes (such as 'and', or 'is'). Term frequency-inverse document frequency is a technique to downweight these frequently occuring words.\n",
    "\n",
    "tf-idf(d,f) can be defined as the product of the term frequency and the inverse document frequency (tf(t,d) * idf(t,d)).\n",
    "\n",
    "idf(t,d) = log(n/(1+df(t,d)); where n is the total number of documents and df(t,d) is the number of documents that contain the term t.\n",
    "idf is small if the term occurs in many documents and thus probably has little discriminative value.\n",
    "\n",
    "Can be implemented vai TfidfTransformer() class of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf = True,\n",
    "                        norm = 'l2',\n",
    "                        smooth_idf = True)\n",
    "np.set_printoptions(precision = 2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text data\n",
    "\n",
    "Before we can implement the above models on the IMDB database we have to lean the text data (for example HTML codes). For simplicity we will now delte punctuation marks, although these can be useful in some cases.\n",
    "\n",
    "Python's regular expression (regex) library, re, can help with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', \"\", text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "#Many programmers advise against the use of regex to parse HTML markup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing documents into tokens\n",
    "\n",
    "The dataset is now prepared, now we want to split the text in to 'tokens' by tokenizing it(splitting the cleaned documents at its whitespace characters). Another useful techniques are: word stemming (processing a word into its root form) and stop-word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizer\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "#PorterStemmr - word stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "tokenizer_porter('runners like running and thus they run')\n",
    "#alternative stemmers are 'Snowball stemmer' (Porter2 or English stemmer) and 'Lancaster stemmer' (Paice/Husk stemmer)\n",
    "#Both are available via the NLTK packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rikkr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stop word removal\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traing a LR-model for document classification\n",
    "\n",
    "Below movie reviews will be classified into positive and negative reviews by logistic regression after dividing the DataFrame into 25,000 documents for training and 25,000 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    }
   ],
   "source": [
    "#Use grid-search to find the optimal set of parameters using 5-fold (stratified) cross validation.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents = None,\n",
    "                       lowercase = False, \n",
    "                       preprocessor = None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range' : [(1,1)],\n",
    "              'vect__stop_words' : [stop, None],\n",
    "              'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "              'clf__penalty' : ['l1', 'l2'],\n",
    "              'clf__C': [1.0, 10.0, 100.0]},\n",
    "             {'vect__ngram_range' : [(1,1)],\n",
    "              'vect__stop_words' : [stop, None],\n",
    "              'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "              'vect__use_idf' : [False],\n",
    "              'vect__norm' : [None],\n",
    "              'clf__penalty' : ['l1', 'l2'],\n",
    "              'clf__C': [1.0, 10.0, 100.0]}]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                    ('clf', LogisticRegression(random_state = 0))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                          scoring = 'accuracy',\n",
    "                          cv = 5, verbose = 1,\n",
    "                          n_jobs = 1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)\n",
    "clf = gs_lr_tfidf.best_estmator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with bigger data - onine algorithms and out-of-core learning\n",
    "\n",
    "As we encountered; constructing feature vectors for large databases is computationally expensive. It is not uncommon to work with even larger datasets in real-world applications (een exceeding the computer's memory). \n",
    "We can apply a technique called Out-of-core learning, which allows us to work with large datasets by fitting the classifier incrementally on smaller batchers of the dataset (directly from the csv-file stored on the harddisk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First a tokenizer function that cleans unprocessed text data from the movie_data.csv file.\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', \"\", text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "#define a generator that reads in and returns one document at a time\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding = 'utf-8') as csv:\n",
    "        next(csv) #skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test stream_docs\n",
    "next(stream_docs(path = 'movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function get_minibatch that will take a document streaam from the stream_docs \n",
    "# and return a particular umber of documents (specified by the size parameter)\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    \n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer requires holding the complete vocabulary in memroy and can thus not be used for out-of-core learning.\n",
    "#Same with TfidfVectorizer, therefore we use HashingVectorizer.\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error = 'ignore',\n",
    "                        n_features = 2**21,\n",
    "                        preprocessor = None,\n",
    "                        tokenizer = tokenizer)\n",
    "clf = SGDClassifier(loss = 'log', random_state = 1, max_iter = 1)\n",
    "# = classifier (LR by setting loss to 'log') using Stochastic Gradient Descent (SGD)\n",
    "# SGD is using one document at a time\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:54\n"
     ]
    }
   ],
   "source": [
    "#Implement the out-of-core model\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0,1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size = 1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes = classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "#eveluate performance of the model using 5000 documents\n",
    "X_test, y_test = get_minibatch(doc_stream, size = 5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update model with those 5000 documents\n",
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "A more modern approach to the bag-of-words model is the word2vec algorithm that learns relationship between words to solve problems like king-man + woman-queen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling with Latent Dirichlet Allocation\n",
    "\n",
    "Topic modeling is the task of assigning topics to unlabelled text documents; it is a clustering task and a subcategory of unsupervised learning. A concrete example is the categroziation of large text corpus of newspaper articles into categories (sports, finance etc.).\n",
    "\n",
    "LDA tries to find groups of words that appear frequently together across different documents. it composes the bag-of-words model into two matrices; 1) a document to topic matrix and 2) a word to topic matrix. (LDA decomposes the bag of words so that the two matrices multiplied reproduce the bag-of-words matrix with the lowest error possible). The number of topics is a hyperparameter that can be defined beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latend Dirichlet Allocation with scikit-learn\n",
    "import pandas as pd\n",
    "df = pd.read_csv('movie_data.csv', encoding = 'utf-8')\n",
    "\n",
    "#Create CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words = 'english',\n",
    "                       max_df = .1, #10%, so that words occuring to too frequently are excluded\n",
    "                       max_features = 5000) #most frequently occuring 5000 words\n",
    "X = count.fit_transform(df['review'].values)\n",
    "\n",
    "#LDA with 10 different topics from the document\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatendDirichletAllocation(n_topics = 10,\n",
    "                               random_state = 123,\n",
    "                               learning_method = 'batch') #Batch: All training examples in one, can also be set to 'online'\n",
    "X_topics = lda.fit_transform(X)\n",
    "\n",
    "#print shape of lda components\n",
    "print(lda.comonents_.shape)\n",
    "\n",
    "#Print the 5 most important words for each of the 10 topics.\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d: \" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                   for i in topic.argsort()[:-n_top_words - 1:1]]))\n",
    "    \n",
    "#plot three horror movies reviews\n",
    "horror = X_topics[:,5].argsort()[::-1]\n",
    "for iter_idx, movie in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie#%d:' (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
