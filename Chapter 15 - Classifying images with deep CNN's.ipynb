{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 - Classifying images with deep CNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d Implementation: [ 5. 14. 16. 26. 24. 34. 19. 22.]\n",
      "Numpy Results: [ 5 14 16 26 24 34 19 22]\n"
     ]
    }
   ],
   "source": [
    "#1d convulution, can be done with hand or using np.convolve\n",
    "import numpy as np\n",
    "def conv1d(x, w, p=0, s=1):\n",
    "    w_rot = np.array(w[::-1])\n",
    "    x_padded = np.array(x)\n",
    "    if p > 0:\n",
    "        zero_pad = np.zeros(shape = p)\n",
    "        x_padded = np.concatenate([zero_pad, x_padded, zero_pad])\n",
    "        \n",
    "    res = []\n",
    "    for i in range(0, int(len(x)/s),s):\n",
    "        res.append(np.sum(x_padded[i:i+w_rot.shape[0]] * w_rot))\n",
    "    \n",
    "    return np.array(res)\n",
    "\n",
    "#testing\n",
    "x = [1, 3, 2, 4, 5, 6, 1, 3]\n",
    "w = [1, 0, 3, 1, 2]\n",
    "\n",
    "print('Conv1d Implementation:', conv1d(x, w, p=2, s=1))\n",
    "print('Numpy Results:', np.convolve(x, w, mode = 'same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Implementation: \n",
      " [[11. 25. 32. 13.]\n",
      " [19. 25. 24. 13.]\n",
      " [13. 28. 25. 17.]\n",
      " [11. 17. 14.  9.]]\n",
      "SciPy Results:\n",
      " [[11 25 32 13]\n",
      " [19 25 24 13]\n",
      " [13 28 25 17]\n",
      " [11 17 14  9]]\n"
     ]
    }
   ],
   "source": [
    "#convulation of 2x2 matrix\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "def conv2d(X, W, p = (0,0), s = (1,1)):\n",
    "    W_rot = np.array(W)[::-1,::-1]\n",
    "    X_orig = np.array(X)\n",
    "    n1 = X_orig.shape[0] + 2*p[0]\n",
    "    n2 = X_orig.shape[1] + 2*p[1]\n",
    "    X_padded = np.zeros(shape=(n1,n2))\n",
    "    X_padded[p[0]:p[0] + X_orig.shape[0],\n",
    "               p[1]:p[1] + X_orig.shape[1]] = X_orig\n",
    "    \n",
    "    res = []\n",
    "    for i in range(0, int((X_padded.shape[0] - W_rot.shape[0])/s[0])+1, s[0]):\n",
    "        res.append([])\n",
    "        for j in range(0, int((X_padded.shape[1] - W_rot.shape[1])/s[1])+1, s[1]):\n",
    "            X_sub = X_padded[i:i+W_rot.shape[0],\n",
    "                            j:j+W_rot.shape[1]]\n",
    "            res[-1].append(np.sum(X_sub*W_rot))\n",
    "    return(np.array(res))\n",
    "\n",
    "X = [[1,3,2,4], [5,6,1,3], [1,2,0,2], [3,4,3,2]]\n",
    "W = [[1,0,3], [1,2,1], [0,1,1]]\n",
    "\n",
    "print('Conv2d Implementation: \\n',\n",
    "     conv2d(X,W, p = (1,1), s = (1,1)))\n",
    "print('SciPy Results:\\n', scipy.signal.convolve2d(X, W, mode = 'same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "for example max/ or mean/average pooling. Pooling introduces some sort of local invariance. This means that small changes in local neighborhood do not change the results. Therefore it makes the data more robust to noise. In addition, pooling decreases the size of features, which results in higher computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (252, 221, 3)\n",
      "Number of channels: 3\n",
      "Image data type: uint8\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "img = imageio.imread('./example-image.png', pilmode = 'RGB')\n",
    "print('Image shape:', img.shape)\n",
    "print('Number of channels:', img.shape[2])\n",
    "print('Image data type:', img.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RGB images can be read as unit8 as these images store values in in a range between 0,255 which is sufficient for RGB images (and computationally effective compared to 16 or 32 bit int."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a deep convolutional NN using TF\n",
    "\n",
    "Architecture\n",
    "* Input [batchsize*28*28*1]\n",
    "* Conv_1 [batchsize*24*24*32]\n",
    "* Pooling_1 [batchsize*12*12*32]\n",
    "* Conv_2 [batchsize*8*8*64]\n",
    "* Pooling_2 [batchsize*4*4*128]\n",
    "* FC_1 [batchsize*1024]\n",
    "* FC_2 and softmax layer [batchsize * 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000,  Columns: 784\n",
      "Rows: 10000,  Columns: 784\n",
      "Training:    (50000, 784) (50000,)\n",
      "Validation:  (10000, 784) (10000,)\n",
      "Test Set:    (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels.idx1-ubyte'\n",
    "                                % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images.idx3-ubyte'\n",
    "                               % kind)\n",
    "\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\",\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "X_data, y_data = load_mnist('./MNIST', kind='train')\n",
    "print('Rows: %d,  Columns: %d' % (X_data.shape[0], X_data.shape[1]))\n",
    "X_test, y_test = load_mnist('./MNIST', kind='t10k')\n",
    "print('Rows: %d,  Columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "X_train, y_train = X_data[:50000,:], y_data[:50000]\n",
    "X_valid, y_valid = X_data[50000:,:], y_data[50000:]\n",
    "\n",
    "print('Training:   ', X_train.shape, y_train.shape)\n",
    "print('Validation: ', X_valid.shape, y_valid.shape)\n",
    "print('Test Set:   ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000,  Columns: 784\n",
      "Rows: 10000,  Columns: 784\n",
      "Training:    (50000, 784) (50000,)\n",
      "Validation:  (10000, 784) (10000,)\n",
      "Test Set:    (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data = load_mnist('./MNIST', kind='train')\n",
    "print('Rows: %d,  Columns: %d' % (X_data.shape[0], X_data.shape[1]))\n",
    "X_test, y_test = load_mnist('./MNIST', kind='t10k')\n",
    "print('Rows: %d,  Columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "X_train, y_train = X_data[:50000,:], y_data[:50000]\n",
    "X_valid, y_valid = X_data[50000:,:], y_data[50000:]\n",
    "\n",
    "print('Training:   ', X_train.shape, y_train.shape)\n",
    "print('Validation: ', X_valid.shape, y_valid.shape)\n",
    "print('Test Set:   ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=64, \n",
    "                    shuffle=False, random_seed=None):\n",
    "    \n",
    "    idx = np.arange(y.shape[0])\n",
    "    \n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        rng.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "    \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X[i:i+batch_size, :], y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize data\n",
    "mean_vals = np.mean(X_train, axis = 0)\n",
    "std_vals = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals)/std_vals\n",
    "X_valid_centered = (X_valid - mean_vals)/std_vals\n",
    "X_test_centered = (X_test - mean_vals)/std_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0820 08:07:10.453307  8440 deprecation.py:506] From C:\\Users\\rikkr\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'convtest/_weights:0' shape=(3, 3, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'convtest/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"convtest/Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convtest/net_pre-activation:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convtest/activation:0\", shape=(?, 28, 28, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## wrapper functions \n",
    "\n",
    "def conv_layer(input_tensor, name,\n",
    "               kernel_size, n_output_channels, \n",
    "               padding_mode='SAME', strides=(1, 1, 1, 1)):\n",
    "    with tf.variable_scope(name):\n",
    "        ## get n_input_channels:\n",
    "        ##   input tensor shape: \n",
    "        ##   [batch x width x height x channels_in]\n",
    "        input_shape = input_tensor.get_shape().as_list()\n",
    "        n_input_channels = input_shape[-1] \n",
    "\n",
    "        weights_shape = (list(kernel_size) + \n",
    "                         [n_input_channels, n_output_channels])\n",
    "\n",
    "        weights = tf.get_variable(name='_weights',\n",
    "                                  shape=weights_shape)\n",
    "        print(weights)\n",
    "        biases = tf.get_variable(name='_biases',\n",
    "                                 initializer=tf.zeros(\n",
    "                                     shape=[n_output_channels]))\n",
    "        print(biases)\n",
    "        conv = tf.nn.conv2d(input=input_tensor, \n",
    "                            filter=weights,\n",
    "                            strides=strides, \n",
    "                            padding=padding_mode)\n",
    "        print(conv)\n",
    "        conv = tf.nn.bias_add(conv, biases, \n",
    "                              name='net_pre-activation')\n",
    "        print(conv)\n",
    "        conv = tf.nn.relu(conv, name='activation')\n",
    "        print(conv)\n",
    "        \n",
    "        return conv\n",
    "    \n",
    "\n",
    "## testing\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    conv_layer(x, name='convtest', kernel_size=(3, 3), n_output_channels=32)\n",
    "    \n",
    "del g, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'convtest/_weights:0' shape=(3, 3, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'convtest/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"convtest/Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convtest/net_pre-activation:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"convtest/activation:0\", shape=(?, 28, 28, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    conv_layer(x, name = 'convtest',\n",
    "              kernel_size = (3,3),\n",
    "              n_output_channels = 32)\n",
    "\n",
    "del g,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'fctest/_weights:0' shape=(784, 32) dtype=float32_ref>\n",
      "<tf.Variable 'fctest/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"fctest/MatMul:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"fctest/net_pre-activation:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"fctest/activation:0\", shape=(?, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def fc_layer(input_tensor, name, \n",
    "             n_output_units, activation_fn=None):\n",
    "    with tf.variable_scope(name):\n",
    "        input_shape = input_tensor.get_shape().as_list()[1:]\n",
    "        n_input_units = np.prod(input_shape)\n",
    "        if len(input_shape) > 1:\n",
    "            input_tensor = tf.reshape(input_tensor, \n",
    "                                      shape=(-1, n_input_units))\n",
    "\n",
    "        weights_shape = [n_input_units, n_output_units]\n",
    "\n",
    "        weights = tf.get_variable(name='_weights',\n",
    "                                  shape=weights_shape)\n",
    "        print(weights)\n",
    "        biases = tf.get_variable(name='_biases',\n",
    "                                 initializer=tf.zeros(\n",
    "                                     shape=[n_output_units]))\n",
    "        print(biases)\n",
    "        layer = tf.matmul(input_tensor, weights)\n",
    "        print(layer)\n",
    "        layer = tf.nn.bias_add(layer, biases,\n",
    "                              name='net_pre-activation')\n",
    "        print(layer)\n",
    "        if activation_fn is None:\n",
    "            return layer\n",
    "        \n",
    "        layer = activation_fn(layer, name='activation')\n",
    "        print(layer)\n",
    "        return layer\n",
    "\n",
    "    \n",
    "## testing:\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, \n",
    "                       shape=[None, 28, 28, 1])\n",
    "    fc_layer(x, name='fctest', n_output_units=32, \n",
    "             activation_fn=tf.nn.relu)\n",
    "    \n",
    "del g, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'fctest/_weights:0' shape=(784, 32) dtype=float32_ref>\n",
      "<tf.Variable 'fctest/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"fctest/MatMul:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"fctest/net_pre-activation:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"fctest/activation:0\", shape=(?, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape = [None, 28,28,1])\n",
    "    fc_layer(x, name = 'fctest', n_output_units=32, activation_fn = tf.nn.relu)\n",
    "    \n",
    "del g, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the cnn\n",
    "def build_cnn(learning_rate = 1e-4):\n",
    "    #placeholders\n",
    "    tf_x = tf.placeholder(tf.float32, shape = [None, 784], name = 'tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32, shape = [None], name = 'tf_y')\n",
    "    \n",
    "    #rehsape x to 4D Tensor\n",
    "    #[batchsize, width, height, 1]\n",
    "    tf_x_image = tf.reshape(tf_x, shape = [-1, 28,28,1], name = 'tf_x_reshaped')\n",
    "    \n",
    "    #One hot encoding\n",
    "    tf_y_onehot = tf.one_hot(indices=tf_y, depth = 10, dtype = tf.float32, name = 'tf_y_onehot')\n",
    "    \n",
    "    ##1st layer: COnv_1\n",
    "    print('\\n Building 1st layer:')\n",
    "    h1 = conv_layer(tf_x_image, name = 'conv_1', kernel_size = (5,5), padding_mode = 'VALID', n_output_channels = 32)\n",
    "    \n",
    "    ##Max pooling\n",
    "    h1_pool = tf.nn.max_pool(h1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME', name = 'maxpool1')\n",
    "    \n",
    "    ##2nd layer: Conv_2\n",
    "    print('\\n Building 2nd layer:')\n",
    "    h2 = conv_layer(h1_pool, name = 'conv_2', kernel_size = (5,5), padding_mode = 'VALID', n_output_channels = 64)\n",
    "    \n",
    "    ##Max pooling\n",
    "    h2_pool = tf.nn.max_pool(h2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME', name = 'maxpool2')\n",
    "    \n",
    "    #3rd layer: Fully connected\n",
    "    print('\\nBuilding 3rd layer:')\n",
    "    h3 = fc_layer(h2_pool, name = 'fc_3', n_output_units = 1024, activation_fn = tf.nn.relu)\n",
    "    \n",
    "    ##Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'fc_keep_prob')\n",
    "    h3_drop = tf.nn.dropout(h3, keep_prob = keep_prob, name = 'dropout_layer')\n",
    "    \n",
    "    ## 4th layer: Fully connected (linear activation)\n",
    "    print('\\nBuilding 4th layer:')\n",
    "    h4 = fc_layer(h3_drop, name = 'fc_4', n_output_units = 10, activation_fn = None)\n",
    "    \n",
    "    ##Predictions\n",
    "    predictions = {\n",
    "        'probabilities' : tf.nn.softmax(h4, name = 'probabilities'),\n",
    "        'labels': tf.cast(tf.argmax(h4, axis = 1), tf.int32, name = 'labels')\n",
    "    }\n",
    "    \n",
    "    ##Loss function and optimzation\n",
    "    cross_entropy_loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits = h4, labels = tf_y_onehot),\n",
    "        name = 'cross_entropy_loss')\n",
    "    \n",
    "    #Optimzer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(cross_entropy_loss, name = 'train_op')\n",
    "    \n",
    "    ##Computing the predictions accuracy\n",
    "    correct_predictions = tf.equal(predictions['labels'], tf_y, name = 'correct_preds')\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(saver, sess, epoch, path='./model/'):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    print('Saving model in %s' % path)\n",
    "    saver.save(sess, os.path.join(path,'cnn-model.ckpt'),\n",
    "               global_step=epoch)\n",
    "\n",
    "    \n",
    "def load(saver, sess, path, epoch):\n",
    "    print('Loading model from %s' % path)\n",
    "    saver.restore(sess, os.path.join(\n",
    "            path, 'cnn-model.ckpt-%d' % epoch))\n",
    "\n",
    "    \n",
    "def train(sess, training_set, validation_set=None,\n",
    "          initialize=True, epochs=20, shuffle=True,\n",
    "          dropout=0.5, random_seed=None):\n",
    "\n",
    "    X_data = np.array(training_set[0])\n",
    "    y_data = np.array(training_set[1])\n",
    "    training_loss = []\n",
    "\n",
    "    ## initialize variables\n",
    "    if initialize:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    np.random.seed(random_seed) # for shuflling in batch_generator\n",
    "    for epoch in range(1, epochs+1):\n",
    "        batch_gen = batch_generator(\n",
    "                        X_data, y_data, \n",
    "                        shuffle=shuffle)\n",
    "        avg_loss = 0.0\n",
    "        for i,(batch_x,batch_y) in enumerate(batch_gen):\n",
    "            feed = {'tf_x:0': batch_x, \n",
    "                    'tf_y:0': batch_y, \n",
    "                    'fc_keep_prob:0': dropout}\n",
    "            loss, _ = sess.run(\n",
    "                    ['cross_entropy_loss:0', 'train_op'],\n",
    "                    feed_dict=feed)\n",
    "            avg_loss += loss\n",
    "\n",
    "        training_loss.append(avg_loss / (i+1))\n",
    "        print('Epoch %02d Training Avg. Loss: %7.3f' % (\n",
    "            epoch, avg_loss), end=' ')\n",
    "        if validation_set is not None:\n",
    "            feed = {'tf_x:0': validation_set[0],\n",
    "                    'tf_y:0': validation_set[1],\n",
    "                    'fc_keep_prob:0':1.0}\n",
    "            valid_acc = sess.run('accuracy:0', feed_dict=feed)\n",
    "            print(' Validation Acc: %7.3f' % valid_acc)\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "            \n",
    "def predict(sess, X_test, return_proba=False):\n",
    "    feed = {'tf_x:0': X_test, \n",
    "            'fc_keep_prob:0': 1.0}\n",
    "    if return_proba:\n",
    "        return sess.run('probabilities:0', feed_dict=feed)\n",
    "    else:\n",
    "        return sess.run('labels:0', feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Building 1st layer:\n",
      "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"conv_1/Conv2D:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/net_pre-activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "\n",
      " Building 2nd layer:\n",
      "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32_ref>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 08:07:10.718598  8440 deprecation.py:506] From <ipython-input-12-da56e103817f>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32_ref>\n",
      "Tensor(\"conv_2/Conv2D:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/net_pre-activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "\n",
      "Building 3rd layer:\n",
      "<tf.Variable 'fc_3/_weights:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc_3/_biases:0' shape=(1024,) dtype=float32_ref>\n",
      "Tensor(\"fc_3/MatMul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/net_pre-activation:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/activation:0\", shape=(?, 1024), dtype=float32)\n",
      "\n",
      "Building 4th layer:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 08:07:10.753504  8440 deprecation.py:323] From <ipython-input-12-da56e103817f>:48: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<tf.Variable 'fc_4/_weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'fc_4/_biases:0' shape=(10,) dtype=float32_ref>\n",
      "Tensor(\"fc_4/MatMul:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc_4/net_pre-activation:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "## Define random seed\n",
    "random_seed = 123\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "## create a graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    ## build the graph\n",
    "    build_cnn()\n",
    "    \n",
    "    #Create graph\n",
    "    file_writer = tf.summary.FileWriter(logdir='./logs/', graph = g)\n",
    "\n",
    "    ## saver:\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 Training Avg. Loss: 273.930  Validation Acc:   0.975\n",
      "Epoch 02 Training Avg. Loss:  74.840  Validation Acc:   0.982\n",
      "Epoch 03 Training Avg. Loss:  52.032  Validation Acc:   0.986\n",
      "Epoch 04 Training Avg. Loss:  39.439  Validation Acc:   0.986\n",
      "Epoch 05 Training Avg. Loss:  31.870  Validation Acc:   0.988\n",
      "Epoch 06 Training Avg. Loss:  26.404  Validation Acc:   0.989\n",
      "Epoch 07 Training Avg. Loss:  23.792  Validation Acc:   0.988\n",
      "Epoch 08 Training Avg. Loss:  20.259  Validation Acc:   0.991\n",
      "Epoch 09 Training Avg. Loss:  17.445  Validation Acc:   0.989\n",
      "Epoch 10 Training Avg. Loss:  16.318  Validation Acc:   0.991\n",
      "Epoch 11 Training Avg. Loss:  13.152  Validation Acc:   0.992\n",
      "Epoch 12 Training Avg. Loss:  12.091  Validation Acc:   0.991\n",
      "Epoch 13 Training Avg. Loss:  10.568  Validation Acc:   0.992\n",
      "Epoch 14 Training Avg. Loss:  10.421  Validation Acc:   0.992\n",
      "Epoch 15 Training Avg. Loss:   8.069  Validation Acc:   0.993\n",
      "Epoch 16 Training Avg. Loss:   7.446  Validation Acc:   0.992\n",
      "Epoch 17 Training Avg. Loss:   6.366  Validation Acc:   0.992\n",
      "Epoch 18 Training Avg. Loss:   5.587  Validation Acc:   0.993\n",
      "Epoch 19 Training Avg. Loss:   5.715  Validation Acc:   0.991\n",
      "Epoch 20 Training Avg. Loss:   5.625  Validation Acc:   0.993\n",
      "Saving model in ./model/\n"
     ]
    }
   ],
   "source": [
    "## create a TF session \n",
    "## and train the CNN model\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    train(sess, \n",
    "          training_set=(X_train_centered, y_train), \n",
    "          validation_set=(X_valid_centered, y_valid), \n",
    "          initialize=True,\n",
    "          random_seed=123)\n",
    "    save(saver, sess, epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Building 1st layer:\n",
      "<tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "Tensor(\"conv_1/Conv2D:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/net_pre-activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "Tensor(\"conv_1/activation:0\", shape=(?, 24, 24, 32), dtype=float32)\n",
      "\n",
      " Building 2nd layer:\n",
      "<tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'conv_2/_biases:0' shape=(64,) dtype=float32_ref>\n",
      "Tensor(\"conv_2/Conv2D:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/net_pre-activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "Tensor(\"conv_2/activation:0\", shape=(?, 8, 8, 64), dtype=float32)\n",
      "\n",
      "Building 3rd layer:\n",
      "<tf.Variable 'fc_3/_weights:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc_3/_biases:0' shape=(1024,) dtype=float32_ref>\n",
      "Tensor(\"fc_3/MatMul:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/net_pre-activation:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"fc_3/activation:0\", shape=(?, 1024), dtype=float32)\n",
      "\n",
      "Building 4th layer:\n",
      "<tf.Variable 'fc_4/_weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'fc_4/_biases:0' shape=(10,) dtype=float32_ref>\n",
      "Tensor(\"fc_4/MatMul:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"fc_4/net_pre-activation:0\", shape=(?, 10), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 08:08:52.683917  8440 deprecation.py:323] From C:\\Users\\rikkr\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model\n",
      "Test Accuracy: 99.290%\n"
     ]
    }
   ],
   "source": [
    "#Restore a saved model, delete graph g, create a new graph g2 and reload the trained model to do predictions\n",
    "\n",
    "### Calculate prediction accuracy on test set\n",
    "### Restoring the saved model\n",
    "\n",
    "del g\n",
    "\n",
    "## Create graph\n",
    "## and build the model\n",
    "\n",
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    ##Build the graph\n",
    "    build_cnn()\n",
    "    \n",
    "    ##saver:\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "#Create a new session\n",
    "#And restore the model\n",
    "with tf.Session(graph = g2) as sess:\n",
    "    load(saver, sess, epoch = 20, path = './model')\n",
    "    \n",
    "    preds = predict(sess, X_test_centered, return_proba = False)\n",
    "    print('Test Accuracy: %.3f%%' % (100 * np.sum(preds == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model\n",
      "[7 2 1 0 4 1 4 9 5 9]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Predicted labels and probabilities\n",
    "##run the prediction on some test samples\n",
    "np.set_printoptions(precision = 2, suppress = True)\n",
    "\n",
    "with tf.Session(graph = g2) as sess:\n",
    "    load(saver, sess, epoch = 20, path = './model')\n",
    "    \n",
    "    print(predict(sess, X_test_centered[:10], return_proba = False))\n",
    "    print(predict(sess, X_test_centered[:10], return_proba = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./model\n",
      "Epoch 01 Training Avg. Loss:   4.349  Validation Acc:   0.993\n",
      "Epoch 02 Training Avg. Loss:   4.063  Validation Acc:   0.992\n",
      "Epoch 03 Training Avg. Loss:   4.028  Validation Acc:   0.993\n",
      "Epoch 04 Training Avg. Loss:   3.596  Validation Acc:   0.993\n",
      "Epoch 05 Training Avg. Loss:   3.208  Validation Acc:   0.993\n",
      "Epoch 06 Training Avg. Loss:   3.474  Validation Acc:   0.993\n",
      "Epoch 07 Training Avg. Loss:   2.720  Validation Acc:   0.991\n",
      "Epoch 08 Training Avg. Loss:   3.014  Validation Acc:   0.992\n",
      "Epoch 09 Training Avg. Loss:   1.978  Validation Acc:   0.992\n",
      "Epoch 10 Training Avg. Loss:   2.454  Validation Acc:   0.993\n",
      "Epoch 11 Training Avg. Loss:   2.115  Validation Acc:   0.991\n",
      "Epoch 12 Training Avg. Loss:   1.907  Validation Acc:   0.992\n",
      "Epoch 13 Training Avg. Loss:   2.296  Validation Acc:   0.993\n",
      "Epoch 14 Training Avg. Loss:   2.360  Validation Acc:   0.993\n",
      "Epoch 15 Training Avg. Loss:   1.680  Validation Acc:   0.993\n",
      "Epoch 16 Training Avg. Loss:   2.313  Validation Acc:   0.992\n",
      "Epoch 17 Training Avg. Loss:   1.307  Validation Acc:   0.993\n",
      "Epoch 18 Training Avg. Loss:   1.275  Validation Acc:   0.992\n",
      "Epoch 19 Training Avg. Loss:   1.759  Validation Acc:   0.992\n",
      "Epoch 20 Training Avg. Loss:   1.515  Validation Acc:   0.991\n",
      "Saving model in ./model\n",
      "Test Accuracy: 99.180%\n"
     ]
    }
   ],
   "source": [
    "## continue training for 20 more epochs\n",
    "## without re-initializzing :: initizalize - false\n",
    "# Create a new session and restore the model\n",
    "\n",
    "with tf.Session(graph = g2) as sess:\n",
    "    load(saver, sess, epoch = 20, path = './model')\n",
    "    \n",
    "    train(sess,\n",
    "         training_set = (X_train_centered, y_train),\n",
    "         validation_set = (X_valid_centered, y_valid),\n",
    "          initialize = False,\n",
    "          epochs = 20,\n",
    "          random_seed = 123)\n",
    "    \n",
    "    save(saver, sess, epoch = 40, path = './model')\n",
    "    \n",
    "    preds = predict(sess, X_test_centered, return_proba = False)\n",
    "    \n",
    "    print('Test Accuracy: %.3f%%' % (100 * np.sum(preds == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a CNN in the tensorflow layers API\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ConvNN(object):\n",
    "    def __init__(self, batchsize=64,\n",
    "                 epochs=20, learning_rate=1e-4, \n",
    "                 dropout_rate=0.5,\n",
    "                 shuffle=True, random_seed=None):\n",
    "        np.random.seed(random_seed)\n",
    "        self.batchsize = batchsize\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.shuffle = shuffle\n",
    "                \n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            ## set random-seed:\n",
    "            tf.set_random_seed(random_seed)\n",
    "            \n",
    "            ## build the network:\n",
    "            self.build()\n",
    "\n",
    "            ## initializer\n",
    "            self.init_op = \\\n",
    "                tf.global_variables_initializer()\n",
    "\n",
    "            ## saver\n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "        ## create a session\n",
    "        self.sess = tf.Session(graph=g)\n",
    "                \n",
    "    def build(self):\n",
    "        \n",
    "        ## Placeholders for X and y:\n",
    "        tf_x = tf.placeholder(tf.float32, \n",
    "                              shape=[None, 784],\n",
    "                              name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32, \n",
    "                              shape=[None],\n",
    "                              name='tf_y')\n",
    "        is_train = tf.placeholder(tf.bool, \n",
    "                              shape=(),\n",
    "                              name='is_train')\n",
    "\n",
    "        ## reshape x to a 4D tensor: \n",
    "        ##  [batchsize, width, height, 1]\n",
    "        tf_x_image = tf.reshape(tf_x, shape=[-1, 28, 28, 1],\n",
    "                              name='input_x_2dimages')\n",
    "        ## One-hot encoding:\n",
    "        tf_y_onehot = tf.one_hot(indices=tf_y, depth=10,\n",
    "                              dtype=tf.float32,\n",
    "                              name='input_y_onehot')\n",
    "\n",
    "        ## 1st layer: Conv_1\n",
    "        h1 = tf.layers.conv2d(tf_x_image, \n",
    "                              kernel_size=(5, 5), \n",
    "                              filters=32, \n",
    "                              activation=tf.nn.relu)\n",
    "        ## MaxPooling\n",
    "        h1_pool = tf.layers.max_pooling2d(h1, \n",
    "                              pool_size=(2, 2), \n",
    "                              strides=(2, 2))\n",
    "        ## 2n layer: Conv_2\n",
    "        h2 = tf.layers.conv2d(h1_pool, kernel_size=(5,5), \n",
    "                              filters=64, \n",
    "                              activation=tf.nn.relu)\n",
    "        ## MaxPooling \n",
    "        h2_pool = tf.layers.max_pooling2d(h2, \n",
    "                              pool_size=(2, 2), \n",
    "                              strides=(2, 2))\n",
    "\n",
    "        ## 3rd layer: Fully Connected\n",
    "        input_shape = h2_pool.get_shape().as_list()\n",
    "        n_input_units = np.prod(input_shape[1:])\n",
    "        h2_pool_flat = tf.reshape(h2_pool, \n",
    "                              shape=[-1, n_input_units])\n",
    "        h3 = tf.layers.dense(h2_pool_flat, 1024, \n",
    "                              activation=tf.nn.relu)\n",
    "\n",
    "        ## Dropout\n",
    "        h3_drop = tf.layers.dropout(h3, \n",
    "                              rate=self.dropout_rate,\n",
    "                              training=is_train)\n",
    "        \n",
    "        ## 4th layer: Fully Connected (linear activation)\n",
    "        h4 = tf.layers.dense(h3_drop, 10, \n",
    "                              activation=None)\n",
    "\n",
    "        ## Prediction\n",
    "        predictions = {\n",
    "            'probabilities': tf.nn.softmax(h4, \n",
    "                              name='probabilities'),\n",
    "            'labels': tf.cast(tf.argmax(h4, axis=1), \n",
    "                              tf.int32, name='labels')}\n",
    "        \n",
    "        ## Loss Function and Optimization\n",
    "        cross_entropy_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=h4, labels=tf_y_onehot),\n",
    "            name='cross_entropy_loss')\n",
    "        \n",
    "        ## Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        optimizer = optimizer.minimize(cross_entropy_loss,\n",
    "                                       name='train_op')\n",
    "\n",
    "        ## Finding accuracy\n",
    "        correct_predictions = tf.equal(\n",
    "            predictions['labels'], \n",
    "            tf_y, name='correct_preds')\n",
    "        \n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, tf.float32),\n",
    "            name='accuracy')\n",
    "\n",
    "    def save(self, epoch, path='./tflayers-model/'):\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "        print('Saving model in %s' % path)\n",
    "        self.saver.save(self.sess, \n",
    "                        os.path.join(path, 'model.ckpt'),\n",
    "                        global_step=epoch)\n",
    "        \n",
    "    def load(self, epoch, path):\n",
    "        print('Loading model from %s' % path)\n",
    "        self.saver.restore(self.sess, \n",
    "             os.path.join(path, 'model.ckpt-%d' % epoch))\n",
    "        \n",
    "    def train(self, training_set, \n",
    "              validation_set=None,\n",
    "              initialize=True):\n",
    "        ## initialize variables\n",
    "        if initialize:\n",
    "            self.sess.run(self.init_op)\n",
    "\n",
    "        self.train_cost_ = []\n",
    "        X_data = np.array(training_set[0])\n",
    "        y_data = np.array(training_set[1])\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            batch_gen = \\\n",
    "                batch_generator(X_data, y_data, \n",
    "                                 shuffle=self.shuffle)\n",
    "            avg_loss = 0.0\n",
    "            for i, (batch_x,batch_y) in \\\n",
    "                enumerate(batch_gen):\n",
    "                feed = {'tf_x:0': batch_x, \n",
    "                        'tf_y:0': batch_y,\n",
    "                        'is_train:0': True} ## for dropout\n",
    "                loss, _ = self.sess.run(\n",
    "                        ['cross_entropy_loss:0', 'train_op'], \n",
    "                        feed_dict=feed)\n",
    "                avg_loss += loss\n",
    "                \n",
    "            print('Epoch %02d: Training Avg. Loss: '\n",
    "                  '%7.3f' % (epoch, avg_loss), end=' ')\n",
    "            if validation_set is not None:\n",
    "                feed = {'tf_x:0': batch_x, \n",
    "                        'tf_y:0': batch_y,\n",
    "                        'is_train:0': False} ## for dropout\n",
    "                valid_acc = self.sess.run('accuracy:0',\n",
    "                                          feed_dict=feed)\n",
    "                print('Validation Acc: %7.3f' % valid_acc)\n",
    "            else:\n",
    "                print()\n",
    "                    \n",
    "    def predict(self, X_test, return_proba = False):\n",
    "        feed = {'tf_x:0': X_test,\n",
    "                'is_train:0': False} ## for dropout\n",
    "        if return_proba:\n",
    "            return self.sess.run('probabilities:0',\n",
    "                                 feed_dict=feed)\n",
    "        else:\n",
    "            return self.sess.run('labels:0',\n",
    "                                 feed_dict=feed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 08:10:31.195315  8440 deprecation.py:323] From <ipython-input-19-8f6df128752e>:63: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0820 08:10:31.443673  8440 deprecation.py:323] From <ipython-input-19-8f6df128752e>:67: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0820 08:10:31.566291  8440 deprecation.py:323] From <ipython-input-19-8f6df128752e>:83: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0820 08:10:31.795825  8440 deprecation.py:323] From <ipython-input-19-8f6df128752e>:88: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Training Avg. Loss: 268.570 Validation Acc:   0.875\n",
      "Epoch 02: Training Avg. Loss:  74.659 Validation Acc:   0.938\n",
      "Epoch 03: Training Avg. Loss:  52.260 Validation Acc:   0.938\n",
      "Epoch 04: Training Avg. Loss:  39.475 Validation Acc:   1.000\n",
      "Epoch 05: Training Avg. Loss:  33.259 Validation Acc:   1.000\n",
      "Epoch 06: Training Avg. Loss:  27.794 Validation Acc:   1.000\n",
      "Epoch 07: Training Avg. Loss:  22.607 Validation Acc:   1.000\n",
      "Epoch 08: Training Avg. Loss:  20.239 Validation Acc:   1.000\n",
      "Epoch 09: Training Avg. Loss:  17.094 Validation Acc:   1.000\n",
      "Epoch 10: Training Avg. Loss:  14.963 Validation Acc:   1.000\n",
      "Epoch 11: Training Avg. Loss:  13.567 Validation Acc:   1.000\n",
      "Epoch 12: Training Avg. Loss:  11.026 Validation Acc:   0.938\n",
      "Epoch 13: Training Avg. Loss:  10.850 Validation Acc:   1.000\n",
      "Epoch 14: Training Avg. Loss:   9.199 Validation Acc:   1.000\n",
      "Epoch 15: Training Avg. Loss:   8.078 Validation Acc:   1.000\n",
      "Epoch 16: Training Avg. Loss:   7.241 Validation Acc:   1.000\n",
      "Epoch 17: Training Avg. Loss:   6.008 Validation Acc:   1.000\n",
      "Epoch 18: Training Avg. Loss:   5.956 Validation Acc:   1.000\n",
      "Epoch 19: Training Avg. Loss:   4.680 Validation Acc:   1.000\n",
      "Epoch 20: Training Avg. Loss:   4.804 Validation Acc:   1.000\n",
      "Saving model in ./tflayers-model/\n"
     ]
    }
   ],
   "source": [
    "cnn = ConvNN(random_seed=123)\n",
    "\n",
    "#Train and save after 20 epochs\n",
    "cnn.train(training_set=(X_train_centered, y_train), \n",
    "          validation_set=(X_valid_centered, y_valid))\n",
    "\n",
    "cnn.save(epoch=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.35%\n"
     ]
    }
   ],
   "source": [
    "preds = cnn.predict(X_test_centered)\n",
    "\n",
    "print('Test Accuracy: %.2f%%' % (100*\n",
    "      np.sum(y_test == preds)/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
