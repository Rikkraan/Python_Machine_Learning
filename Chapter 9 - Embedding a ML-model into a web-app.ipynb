{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 - Embedding a ML-model into a web application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For web applications it is not feasible to learn from the training data every single time a prediction has to be made. Therefore, we an save the current state of the model into a pickle opbject in Python's built in Picle model. \n",
    "\n",
    "For this purpose we load the out-of-core LR-model from chapter 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First a tokenizer function that cleans unprocessed text data from the movie_data.csv file.\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', \"\", text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "#define a generator that reads in and returns one document at a time\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding = 'utf-8') as csv:\n",
    "        next(csv) #skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label\n",
    "            \n",
    "#Define function get_minibatch that will take a document streaam from the stream_docs \n",
    "# and return a particular umber of documents (specified by the size parameter)\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    \n",
    "    return docs, y            \n",
    "\n",
    "#CountVectorizer requires holding the complete vocabulary in memroy and can thus not be used for out-of-core learning.\n",
    "#Same with TfidfVectorizer, therefore we use HashingVectorizer.\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error = 'ignore',\n",
    "                        n_features = 2**21,\n",
    "                        preprocessor = None,\n",
    "                        tokenizer = tokenizer)\n",
    "clf = SGDClassifier(loss = 'log', random_state = 1, max_iter = 1)\n",
    "# = classifier (LR by setting loss to 'log') using Stochastic Gradient Descent (SGD)\n",
    "# SGD is using one document at a time\n",
    "doc_stream = stream_docs(path='movie_data.csv')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:53\n"
     ]
    }
   ],
   "source": [
    "#Implement the out-of-core model\n",
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0,1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size = 1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes = classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "#eveluate performance of the model using 5000 documents\n",
    "X_test, y_test = get_minibatch(doc_stream, size = 5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update model with those 5000 documents\n",
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save current state of ML-model with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "#Create a folder if it not eixsts.\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "#Save stopword set, so the NLTK library doest not have to be installed on the server\n",
    "pickle.dump(stop,\n",
    "           open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol = 4)\n",
    "#Save model\n",
    "pickle.dump(clf,\n",
    "           open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol = 4) #wb = binary mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and stopword set are now saved to a newly created folder. Another option is to use th joblib library (may be more efficient).\n",
    "\n",
    "The Hashingvectorizer we created in chapter 8 does not have to be fitted an therefore it is not necessary to store it via pickle. We are going to create a new .py file with the hashingvectorizer code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading vectorizer, preprocess documents and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rikkr\\Documents\\R\\Python Machine Learning\\movieclassifier\n"
     ]
    }
   ],
   "source": [
    "%cd c:\\Users\\rikkr\\Documents\\R\\Python Machine Learning\\movieclassifier\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from vectorizer import vect\n",
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 82.52%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "label = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "example = ['I love this movie']\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%' % (label[clf.predict(X)[0]], np.max(clf.predict_proba(X)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up an SQLite database for data storage\n",
    "\n",
    "Database to collect optional feedback about predictions from users of the web application. We can then use this feedback to update our classification model. SQLite is open source and does not require a seperate server to operate on. In Python already an integrated API exists for SQLite --> sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "if os.path.exists('reviews.sqlite'):\n",
    "    os.remove('reviews.sqlite')\n",
    "conn = sqlite3.connect('reviews.sqlite') #Connect to SQLite dataabse and create new database file\n",
    "c = conn.cursor() #Create a cursor (allows us to traverse over database records using SQL syntax)\n",
    "c.execute('CREATE TABLE review_db' ' (review TEXT, sentiment INTEGER, data TEXT)') #Create table with 3 columns\n",
    "\n",
    "#Store 2 examples (with classlabels 1 and 0)\n",
    "example1 = 'I love this movie'\n",
    "c.execute(\"INSERT INTO review_db\"\\\n",
    "          \"(review, sentiment, data) VALUES\"\\\n",
    "          \"(?,?, DATETIME('now'))\", (example1, 1)) \n",
    "\n",
    "example2 = 'I disliked this movie'\n",
    "c.execute(\"INSERT INTO review_db\"\\\n",
    "          \" (review, sentiment, data) VALUES\"\\\n",
    "          \" (?,?, DATETIME('now'))\", (example2, 0))\n",
    "\n",
    "conn.commit() #Save changes\n",
    "conn.close() #Close connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I love this movie', 1, '2019-08-11 06:51:28'), ('I disliked this movie', 0, '2019-08-11 06:51:28')]\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('reviews.sqlite') #connect to the databse\n",
    "c = conn.cursor() #Create a cursor to traverse over records in database\n",
    "c.execute(\"SELECT * FROM review_db WHERE data\"\\ #Select all recorts in review_db between 1-1-2017 and current date/time\n",
    "        \" BETWEEN '2017-01-01 00:00:00' AND DATETIME('now')\")\n",
    "\n",
    "results = c.fetchall()\n",
    "conn.close()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a GUI interface for working with SQLite databases can be found as Firefox plugin at https://addons.mozilla.org/en-US/firefox/addon/sqlite-manager/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a web application with Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flask is konown as a micro-framework which means that its core is lean and simple, but can easily be extended with other libraries (an alternative to Flask is Django). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first Flask web-application\n",
    "\n",
    "We are going to build a simple web application with a form field that lets us enter a name. After entering a name, it will render it on a new page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a flask application the directory tree is as follows:\n",
    "\n",
    "1st_flask_app_1/ <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;app.py <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;templates/ <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first_app.html\n",
    "\n",
    "<br /><br /><br /><br />\n",
    "Code in the app.py file:\n",
    "\n",
    "from flask import Flask, render_template<br /><br />\n",
    "app = Flask(__name__)<br />\n",
    "@app.route('/')<br /><br />\n",
    "def index():<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;return render_template('first_app.html')<br /><br />\n",
    "if __name__ == '__main__':<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@app.route() <-- insert which url should trigger the function below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check folders on local drive and textbook for more info on web-applications with Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the app to a public server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pythonanywhere we can deploy a single web application free of charge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the movieclassifier app the model is partially_fitted every time feedback is provided to the model. However, as the code does not include pickling of the model, all progress is lost if the servers crash. One method would be to save the model als pickle object after each feedback-loop, however this is not computationally efficient and may corrupt the pkl file if multiple users try to update the file simultaneously.\n",
    "\n",
    "Another (better) option is to update the predictive model with feedback data from the SQlite database. For example downloading the responses from the table to our local computer, updating the clf object, upload the new picle file to the webapp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly update will update the model with entries from the SQL-table. However, in practice the data (userfeedback) should be validated prior to updating the model. In addition --> Overfitting if training on same examples every time???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
